{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading train and test user-item set ...\n",
      "building the adj mat ...\n",
      "{'n_users': 7670, 'n_items': 10887}\n",
      "loading over ...\n",
      "start training ...\n",
      "+-------+--------------------+-------------------+------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |       Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+------------------+--------------+--------------+--------------+--------------+\n",
      "|   0   | 0.7475862503051758 | 2.922084331512451 | 21.4107723236084 | [0.01504579] | [0.00814599] | [0.00237229] | [0.03526942] |\n",
      "+-------+--------------------+-------------------+------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7197s, training loss at epoch 1: 20.5793\n",
      "using time 0.6935s, training loss at epoch 2: 18.5607\n",
      "using time 0.6713s, training loss at epoch 3: 16.3386\n",
      "using time 0.6548s, training loss at epoch 4: 14.7574\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-----------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |    ndcg   |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-----------+--------------+--------------+\n",
      "|   5   | 0.6930153369903564 | 3.049144744873047 | 13.707481384277344 | [0.01621652] | [0.00836] | [0.00262421] | [0.03988803] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-----------+--------------+--------------+\n",
      "using time 0.6825s, training loss at epoch 6: 12.9540\n",
      "using time 0.7500s, training loss at epoch 7: 12.3512\n",
      "using time 0.6755s, training loss at epoch 8: 11.8524\n",
      "using time 0.7017s, training loss at epoch 9: 11.3621\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   10  | 0.6970334053039551 | 3.0251941680908203 | 10.863462448120117 | [0.02157468] | [0.01157444] | [0.00336599] | [0.05206438] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7119s, training loss at epoch 11: 10.4058\n",
      "using time 0.7240s, training loss at epoch 12: 9.9345\n",
      "using time 0.7189s, training loss at epoch 13: 9.5173\n",
      "using time 0.7093s, training loss at epoch 14: 9.1684\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   15  | 0.6765055656433105 | 2.944692611694336 | 8.782357215881348 | [0.02746749] | [0.01456647] | [0.00407978] | [0.06340098] |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7257s, training loss at epoch 16: 8.3714\n",
      "using time 0.6950s, training loss at epoch 17: 7.9622\n",
      "using time 0.7256s, training loss at epoch 18: 7.6873\n",
      "using time 0.6707s, training loss at epoch 19: 7.3932\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   20  | 0.7285239696502686 | 2.964313507080078 | 7.052330493927002 | [0.03012358] | [0.01642723] | [0.00441568] | [0.06871938] |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7324s, training loss at epoch 21: 6.7528\n",
      "using time 0.7186s, training loss at epoch 22: 6.5634\n",
      "using time 0.7102s, training loss at epoch 23: 6.2341\n",
      "using time 0.7154s, training loss at epoch 24: 6.0160\n",
      "+-------+-------------------+------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |  tesing time(s)  |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   25  | 0.677889347076416 | 2.90802264213562 | 5.796746730804443 | [0.03250772] | [0.01755647] | [0.00469559] | [0.07347796] |\n",
      "+-------+-------------------+------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7358s, training loss at epoch 26: 5.6257\n",
      "using time 0.7220s, training loss at epoch 27: 5.3781\n",
      "using time 0.6905s, training loss at epoch 28: 5.2459\n",
      "using time 0.7115s, training loss at epoch 29: 5.0430\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   30  | 0.6894896030426025 | 2.941793918609619 | 4.891352653503418 | [0.03277441] | [0.01811452] | [0.00477257] | [0.07417775] |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7136s, training loss at epoch 31: 4.7189\n",
      "using time 0.6931s, training loss at epoch 32: 4.6687\n",
      "using time 0.6917s, training loss at epoch 33: 4.4194\n",
      "using time 0.6870s, training loss at epoch 34: 4.2392\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+-------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio  |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+-------------+\n",
      "|   35  | 0.7157080173492432 | 2.9303555488586426 | 4.154848098754883 | [0.03409994] | [0.01862278] | [0.00494052] | [0.0776767] |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+-------------+\n",
      "using time 0.7470s, training loss at epoch 36: 4.0022\n",
      "using time 0.6999s, training loss at epoch 37: 3.9639\n",
      "using time 0.6964s, training loss at epoch 38: 3.8154\n",
      "using time 0.6922s, training loss at epoch 39: 3.6616\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   40  | 0.7035503387451172 | 2.9539341926574707 | 3.5780770778656006 | [0.03452269] | [0.01905002] | [0.00494752] | [0.07781666] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7436s, training loss at epoch 41: 3.4920\n",
      "using time 0.7007s, training loss at epoch 42: 3.3972\n",
      "using time 0.6936s, training loss at epoch 43: 3.2901\n",
      "using time 0.6946s, training loss at epoch 44: 3.1866\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision  |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "|   45  | 0.7159624099731445 | 2.9981117248535156 | 3.0829341411590576 | [0.03475611] | [0.01927764] | [0.0050035] | [0.07851645] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time 0.7055s, training loss at epoch 46: 3.0338\n",
      "using time 0.7239s, training loss at epoch 47: 2.9695\n",
      "using time 0.7103s, training loss at epoch 48: 2.8998\n",
      "using time 0.7053s, training loss at epoch 49: 2.8343\n",
      "+-------+--------------------+--------------------+-------------------+--------------+------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |    recall    |    ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+------------+--------------+--------------+\n",
      "|   50  | 0.7137954235076904 | 2.9489128589630127 | 2.712573528289795 | [0.03479958] | [0.019296] | [0.00503149] | [0.07879636] |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+------------+--------------+--------------+\n",
      "using time 0.7762s, training loss at epoch 51: 2.7289\n",
      "using time 0.6709s, training loss at epoch 52: 2.6409\n",
      "using time 0.7031s, training loss at epoch 53: 2.5668\n",
      "using time 0.6969s, training loss at epoch 54: 2.4414\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   55  | 0.7052710056304932 | 2.8777077198028564 | 2.408597707748413 | [0.03419986] | [0.01928322] | [0.00494752] | [0.07795661] |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7663s, training loss at epoch 56: 2.3731\n",
      "using time 0.6938s, training loss at epoch 57: 2.3615\n",
      "using time 0.7337s, training loss at epoch 58: 2.2963\n",
      "using time 0.7026s, training loss at epoch 59: 2.2646\n",
      "+-------+--------------------+-------------------+-------------------+-------------+-------------+-------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |    recall   |     ndcg    |  precision  |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+-------------------+-------------+-------------+-------------+--------------+\n",
      "|   60  | 0.6898272037506104 | 2.968639850616455 | 2.257209062576294 | [0.0348078] | [0.0194307] | [0.0050105] | [0.07879636] |\n",
      "+-------+--------------------+-------------------+-------------------+-------------+-------------+-------------+--------------+\n",
      "using time 0.7495s, training loss at epoch 61: 2.1271\n",
      "using time 0.6900s, training loss at epoch 62: 2.1251\n",
      "using time 0.6985s, training loss at epoch 63: 2.0814\n",
      "using time 0.6984s, training loss at epoch 64: 2.0436\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision  |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "|   65  | 0.7237036228179932 | 2.9404757022857666 | 2.0043299198150635 | [0.03473892] | [0.01946744] | [0.0049965] | [0.07837649] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "using time 0.7857s, training loss at epoch 66: 1.9357\n",
      "using time 0.7146s, training loss at epoch 67: 1.8739\n",
      "using time 0.7334s, training loss at epoch 68: 1.9227\n",
      "using time 0.7160s, training loss at epoch 69: 1.8073\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   70  | 0.731971025466919 | 2.994520664215088 | 1.8680853843688965 | [0.03544355] | [0.01982176] | [0.00507348] | [0.07907628] |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7276s, training loss at epoch 71: 1.7734\n",
      "using time 0.7244s, training loss at epoch 72: 1.7401\n",
      "using time 0.7250s, training loss at epoch 73: 1.7136\n",
      "using time 0.7214s, training loss at epoch 74: 1.6459\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   75  | 0.730165958404541 | 3.031773328781128 | 1.6599154472351074 | [0.03539893] | [0.01979259] | [0.00505248] | [0.07879636] |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7083s, training loss at epoch 76: 1.6362\n",
      "using time 0.7245s, training loss at epoch 77: 1.6028\n",
      "using time 0.7187s, training loss at epoch 78: 1.5889\n",
      "using time 0.7113s, training loss at epoch 79: 1.5786\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   80  | 0.7178857326507568 | 2.9946014881134033 | 1.5248417854309082 | [0.03585843] | [0.02005808] | [0.00508747] | [0.07963611] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7247s, training loss at epoch 81: 1.5251\n",
      "using time 0.6850s, training loss at epoch 82: 1.4843\n",
      "using time 0.7573s, training loss at epoch 83: 1.4347\n",
      "using time 0.6824s, training loss at epoch 84: 1.4752\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   85  | 0.6963460445404053 | 3.0040881633758545 | 1.425435185432434 | [0.03628866] | [0.02017112] | [0.00514346] | [0.08075577] |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7057s, training loss at epoch 86: 1.4337\n",
      "using time 0.6963s, training loss at epoch 87: 1.3771\n",
      "using time 0.7045s, training loss at epoch 88: 1.3751\n",
      "using time 0.6905s, training loss at epoch 89: 1.3225\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   90  | 0.6754281520843506 | 3.007755994796753 | 1.3271446228027344 | [0.03593162] | [0.02010976] | [0.00512946] | [0.07963611] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.6825s, training loss at epoch 91: 1.3132\n",
      "using time 0.7408s, training loss at epoch 92: 1.3094\n",
      "using time 0.7044s, training loss at epoch 93: 1.3080\n",
      "using time 0.7049s, training loss at epoch 94: 1.2314\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   95  | 0.6960837841033936 | 2.984184741973877 | 1.2230383157730103 | [0.03542233] | [0.01995989] | [0.00508747] | [0.07921624] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time 0.7090s, training loss at epoch 96: 1.2314\n",
      "using time 0.6832s, training loss at epoch 97: 1.2264\n",
      "using time 0.7177s, training loss at epoch 98: 1.2260\n",
      "using time 0.6877s, training loss at epoch 99: 1.2109\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|  100  | 0.7056469917297363 | 2.972174644470215 | 1.177783727645874 | [0.03602409] | [0.02016001] | [0.00512946] | [0.07977607] |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7409s, training loss at epoch 101: 1.2027\n",
      "using time 0.6978s, training loss at epoch 102: 1.1461\n",
      "using time 0.6708s, training loss at epoch 103: 1.1487\n",
      "using time 0.6971s, training loss at epoch 104: 1.1185\n",
      "+-------+--------------------+------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |  tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  105  | 0.7039730548858643 | 2.99937105178833 | 1.0835251808166504 | [0.03641617] | [0.02023688] | [0.00519244] | [0.08047586] |\n",
      "+-------+--------------------+------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7222s, training loss at epoch 106: 1.1036\n",
      "using time 0.6779s, training loss at epoch 107: 1.0830\n",
      "using time 0.6800s, training loss at epoch 108: 1.0808\n",
      "using time 0.6923s, training loss at epoch 109: 1.0688\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  110  | 0.7076208591461182 | 3.0195531845092773 | 1.0548510551452637 | [0.03633784] | [0.02012614] | [0.00517145] | [0.08075577] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7672s, training loss at epoch 111: 1.0221\n",
      "using time 0.7212s, training loss at epoch 112: 0.9987\n",
      "using time 0.7087s, training loss at epoch 113: 0.9981\n",
      "using time 0.6792s, training loss at epoch 114: 1.0226\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  115  | 0.7220351696014404 | 2.8718080520629883 | 1.0036015510559082 | [0.03611812] | [0.02015057] | [0.00517145] | [0.08047586] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.6985s, training loss at epoch 116: 0.9829\n",
      "using time 0.6992s, training loss at epoch 117: 0.9326\n",
      "using time 0.6644s, training loss at epoch 118: 0.9794\n",
      "using time 0.6956s, training loss at epoch 119: 0.9606\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  120  | 0.702183723449707 | 2.966384172439575 | 0.9330540895462036 | [0.03638868] | [0.02034207] | [0.00518544] | [0.08047586] |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7125s, training loss at epoch 121: 0.9429\n",
      "using time 0.6893s, training loss at epoch 122: 0.9208\n",
      "using time 0.7019s, training loss at epoch 123: 0.9150\n",
      "using time 0.7034s, training loss at epoch 124: 0.8989\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "|  125  | 0.6846151351928711 | 2.949782609939575 | 0.9115971922874451 | [0.03626043] | [0.0203009] | [0.00517845] | [0.07991603] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "using time 0.7414s, training loss at epoch 126: 0.9139\n",
      "using time 0.7030s, training loss at epoch 127: 0.8892\n",
      "using time 0.6817s, training loss at epoch 128: 0.8399\n",
      "using time 0.7075s, training loss at epoch 129: 0.8472\n",
      "+-------+-------------------+------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |  tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  130  | 0.697613000869751 | 2.94244647026062 | 0.8617485761642456 | [0.03645616] | [0.02037187] | [0.00519944] | [0.07963611] |\n",
      "+-------+-------------------+------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7569s, training loss at epoch 131: 0.8655\n",
      "using time 0.6754s, training loss at epoch 132: 0.8396\n",
      "using time 0.7002s, training loss at epoch 133: 0.8456\n",
      "using time 0.6994s, training loss at epoch 134: 0.8370\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  135  | 0.6968402862548828 | 3.0206589698791504 | 0.8311486840248108 | [0.03598021] | [0.02026629] | [0.00519244] | [0.07963611] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7018s, training loss at epoch 136: 0.8297\n",
      "using time 0.7115s, training loss at epoch 137: 0.7793\n",
      "using time 0.7157s, training loss at epoch 138: 0.7690\n",
      "using time 0.7092s, training loss at epoch 139: 0.8200\n",
      "+-------+--------------------+------------------+--------------------+-------------+------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |  tesing time(s)  |        Loss        |    recall   |    ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+------------------+--------------------+-------------+------------+--------------+--------------+\n",
      "|  140  | 0.7067606449127197 | 3.01432204246521 | 0.7682817578315735 | [0.0362645] | [0.020407] | [0.00521344] | [0.07963611] |\n",
      "+-------+--------------------+------------------+--------------------+-------------+------------+--------------+--------------+\n",
      "using time 0.7610s, training loss at epoch 141: 0.7871\n",
      "using time 0.7048s, training loss at epoch 142: 0.7986\n",
      "using time 0.7107s, training loss at epoch 143: 0.7701\n",
      "using time 0.6980s, training loss at epoch 144: 0.7584\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall   |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "|  145  | 0.7106947898864746 | 2.9865686893463135 | 0.7336624264717102 | [0.0366213] | [0.02038277] | [0.00525542] | [0.08005598] |\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------+--------------+--------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time 0.7040s, training loss at epoch 146: 0.7453\n",
      "using time 0.7063s, training loss at epoch 147: 0.7552\n",
      "using time 0.7247s, training loss at epoch 148: 0.7315\n",
      "using time 0.7033s, training loss at epoch 149: 0.7322\n",
      "+-------+--------------------+-------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall   |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "|  150  | 0.6892476081848145 | 3.000887393951416 | 0.7160650491714478 | [0.0361701] | [0.02033787] | [0.00522743] | [0.07935619] |\n",
      "+-------+--------------------+-------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "using time 0.7307s, training loss at epoch 151: 0.7152\n",
      "using time 0.6862s, training loss at epoch 152: 0.7132\n",
      "using time 0.6778s, training loss at epoch 153: 0.7007\n",
      "using time 0.6833s, training loss at epoch 154: 0.7133\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  155  | 0.7120435237884521 | 2.948451280593872 | 0.7129813432693481 | [0.03598151] | [0.02021769] | [0.00519944] | [0.07879636] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.6963s, training loss at epoch 156: 0.7121\n",
      "using time 0.7043s, training loss at epoch 157: 0.6578\n",
      "using time 0.7502s, training loss at epoch 158: 0.6906\n",
      "using time 0.7034s, training loss at epoch 159: 0.6656\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+-------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio  |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+-------------+\n",
      "|  160  | 0.6527626514434814 | 3.010824680328369 | 0.6494319438934326 | [0.03600268] | [0.02017029] | [0.00519944] | [0.0786564] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+-------------+\n",
      "using time 0.7248s, training loss at epoch 161: 0.6947\n",
      "using time 0.6719s, training loss at epoch 162: 0.6682\n",
      "using time 0.7119s, training loss at epoch 163: 0.6559\n",
      "using time 0.7280s, training loss at epoch 164: 0.6305\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall   |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "|  165  | 0.6934051513671875 | 2.9248509407043457 | 0.6607900261878967 | [0.0362478] | [0.02034099] | [0.00524143] | [0.07949615] |\n",
      "+-------+--------------------+--------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "using time 0.7059s, training loss at epoch 166: 0.6452\n",
      "using time 0.7514s, training loss at epoch 167: 0.6531\n",
      "using time 0.7256s, training loss at epoch 168: 0.6554\n",
      "using time 0.7084s, training loss at epoch 169: 0.6767\n",
      "+-------+--------------------+-----------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |  tesing time(s) |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-----------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  170  | 0.7104387283325195 | 2.9124915599823 | 0.6460700035095215 | [0.03628526] | [0.02028348] | [0.00522743] | [0.07949615] |\n",
      "+-------+--------------------+-----------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.6988s, training loss at epoch 171: 0.6214\n",
      "using time 0.6972s, training loss at epoch 172: 0.6256\n",
      "using time 0.6798s, training loss at epoch 173: 0.5997\n",
      "using time 0.7082s, training loss at epoch 174: 0.5994\n",
      "+-------+--------------------+--------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |    ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|  175  | 0.7062547206878662 | 3.1373178958892822 | 0.6155858635902405 | [0.03628535] | [0.020287] | [0.00523443] | [0.07935619] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "using time 0.7303s, training loss at epoch 176: 0.6275\n",
      "using time 0.7021s, training loss at epoch 177: 0.6155\n",
      "using time 0.7002s, training loss at epoch 178: 0.6301\n",
      "using time 0.7134s, training loss at epoch 179: 0.5928\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  180  | 0.7013916969299316 | 3.001291513442993 | 0.5706678628921509 | [0.03678253] | [0.02055829] | [0.00528341] | [0.08047586] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7228s, training loss at epoch 181: 0.5678\n",
      "using time 0.6904s, training loss at epoch 182: 0.5940\n",
      "using time 0.7004s, training loss at epoch 183: 0.5789\n",
      "using time 0.6773s, training loss at epoch 184: 0.5769\n",
      "+-------+--------------------+--------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |    ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|  185  | 0.6905927658081055 | 2.9980006217956543 | 0.5582553148269653 | [0.03679914] | [0.020505] | [0.00528341] | [0.07977607] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "using time 0.7507s, training loss at epoch 186: 0.5708\n",
      "using time 0.7002s, training loss at epoch 187: 0.5648\n",
      "using time 0.6866s, training loss at epoch 188: 0.5769\n",
      "using time 0.7208s, training loss at epoch 189: 0.5646\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  190  | 0.7232415676116943 | 3.229120969772339 | 0.5747540593147278 | [0.03676245] | [0.02045985] | [0.00525542] | [0.07949615] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7890s, training loss at epoch 191: 0.5547\n",
      "using time 0.7541s, training loss at epoch 192: 0.5232\n",
      "using time 0.7275s, training loss at epoch 193: 0.5601\n",
      "using time 0.7223s, training loss at epoch 194: 0.5635\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  195  | 0.8184804916381836 | 3.338535785675049 | 0.5119308233261108 | [0.03709945] | [0.02045821] | [0.00525542] | [0.08005598] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time 0.8408s, training loss at epoch 196: 0.5329\n",
      "using time 0.8428s, training loss at epoch 197: 0.5275\n",
      "using time 0.7708s, training loss at epoch 198: 0.5335\n",
      "using time 0.7033s, training loss at epoch 199: 0.5208\n",
      "+-------+--------------------+--------------------+--------------------+------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |   recall   |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+------------+--------------+--------------+--------------+\n",
      "|  200  | 0.7057132720947266 | 2.9764344692230225 | 0.5392257571220398 | [0.037488] | [0.02050166] | [0.00529741] | [0.08047586] |\n",
      "+-------+--------------------+--------------------+--------------------+------------+--------------+--------------+--------------+\n",
      "using time 0.7911s, training loss at epoch 201: 0.5141\n",
      "using time 0.7254s, training loss at epoch 202: 0.5109\n",
      "using time 0.8776s, training loss at epoch 203: 0.5110\n",
      "using time 0.8487s, training loss at epoch 204: 0.5050\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  205  | 0.7427527904510498 | 3.272156000137329 | 0.4965061843395233 | [0.03759063] | [0.02040404] | [0.00529041] | [0.08047586] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7831s, training loss at epoch 206: 0.5144\n",
      "using time 0.7303s, training loss at epoch 207: 0.4998\n",
      "using time 0.7469s, training loss at epoch 208: 0.4804\n",
      "using time 0.8363s, training loss at epoch 209: 0.4861\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  210  | 0.9099221229553223 | 3.3424901962280273 | 0.4960462152957916 | [0.03748333] | [0.02041348] | [0.00531141] | [0.08019594] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8439s, training loss at epoch 211: 0.4923\n",
      "using time 0.7573s, training loss at epoch 212: 0.4849\n",
      "using time 0.6803s, training loss at epoch 213: 0.4794\n",
      "using time 0.6967s, training loss at epoch 214: 0.4971\n",
      "+-------+-------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)  |         Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "|  215  | 0.848947286605835 | 3.256453037261963 | 0.46811744570732117 | [0.03739858] | [0.02029017] | [0.00529041] | [0.07949615] |\n",
      "+-------+-------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8166s, training loss at epoch 216: 0.5018\n",
      "using time 0.7924s, training loss at epoch 217: 0.4976\n",
      "using time 0.7420s, training loss at epoch 218: 0.4767\n",
      "using time 0.6834s, training loss at epoch 219: 0.4736\n",
      "+-------+--------------------+--------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |         Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "|  220  | 0.7168643474578857 | 3.1754205226898193 | 0.45748481154441833 | [0.03764997] | [0.02034293] | [0.00528341] | [0.07949615] |\n",
      "+-------+--------------------+--------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7215s, training loss at epoch 221: 0.4837\n",
      "using time 0.7319s, training loss at epoch 222: 0.4878\n",
      "using time 0.8183s, training loss at epoch 223: 0.4599\n",
      "using time 0.8900s, training loss at epoch 224: 0.4488\n",
      "+-------+--------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |         Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "|  225  | 0.7602593898773193 | 3.384182929992676 | 0.47230735421180725 | [0.03787169] | [0.02047241] | [0.00530441] | [0.07963611] |\n",
      "+-------+--------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7483s, training loss at epoch 226: 0.4631\n",
      "using time 0.6792s, training loss at epoch 227: 0.4423\n",
      "using time 0.6604s, training loss at epoch 228: 0.4665\n",
      "using time 0.8445s, training loss at epoch 229: 0.4528\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision  |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "|  230  | 0.8403053283691406 | 3.400972604751587 | 0.4622487723827362 | [0.03791875] | [0.02054251] | [0.0053254] | [0.08005598] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "using time 0.8684s, training loss at epoch 231: 0.4426\n",
      "using time 0.8246s, training loss at epoch 232: 0.4328\n",
      "using time 0.7664s, training loss at epoch 233: 0.4555\n",
      "using time 0.7107s, training loss at epoch 234: 0.4448\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  235  | 0.7062993049621582 | 3.0809385776519775 | 0.4479919672012329 | [0.03755181] | [0.02035625] | [0.00527642] | [0.07921624] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7318s, training loss at epoch 236: 0.4213\n",
      "using time 0.7068s, training loss at epoch 237: 0.4296\n",
      "using time 0.7121s, training loss at epoch 238: 0.4479\n",
      "using time 0.6960s, training loss at epoch 239: 0.4316\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  240  | 0.7250525951385498 | 3.028562307357788 | 0.4380591809749603 | [0.03787388] | [0.02031015] | [0.00531141] | [0.07963611] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7062s, training loss at epoch 241: 0.4431\n",
      "using time 0.7382s, training loss at epoch 242: 0.4254\n",
      "using time 0.6854s, training loss at epoch 243: 0.4134\n",
      "using time 0.7061s, training loss at epoch 244: 0.4312\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "|  245  | 0.7087864875793457 | 3.002650260925293 | 0.4144758880138397 | [0.03731166] | [0.0202248] | [0.00524843] | [0.07851645] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+-------------+--------------+--------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time 0.7315s, training loss at epoch 246: 0.4293\n",
      "using time 0.6979s, training loss at epoch 247: 0.4092\n",
      "using time 0.7122s, training loss at epoch 248: 0.4158\n",
      "using time 0.6812s, training loss at epoch 249: 0.4338\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  250  | 0.7115204334259033 | 3.0247671604156494 | 0.3955393135547638 | [0.03764883] | [0.02015375] | [0.00526942] | [0.07977607] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7572s, training loss at epoch 251: 0.4169\n",
      "using time 0.7046s, training loss at epoch 252: 0.4146\n",
      "using time 0.7054s, training loss at epoch 253: 0.4300\n",
      "using time 0.7048s, training loss at epoch 254: 0.4015\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision  |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "|  255  | 0.6720728874206543 | 3.0359930992126465 | 0.4054967761039734 | [0.03852911] | [0.02054067] | [0.0053394] | [0.08047586] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "using time 0.7105s, training loss at epoch 256: 0.3833\n",
      "using time 0.6676s, training loss at epoch 257: 0.4036\n",
      "using time 0.6985s, training loss at epoch 258: 0.4092\n",
      "using time 0.6964s, training loss at epoch 259: 0.3891\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision  |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "|  260  | 0.6721804141998291 | 3.0551161766052246 | 0.3892018795013428 | [0.03812879] | [0.02033109] | [0.0053184] | [0.07991603] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "using time 0.7559s, training loss at epoch 261: 0.3987\n",
      "using time 0.6973s, training loss at epoch 262: 0.4096\n",
      "using time 0.7155s, training loss at epoch 263: 0.3885\n",
      "using time 0.6984s, training loss at epoch 264: 0.3899\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  265  | 0.719630241394043 | 2.986161231994629 | 0.3997963070869446 | [0.03802788] | [0.02025941] | [0.00527642] | [0.07977607] |\n",
      "+-------+-------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7291s, training loss at epoch 266: 0.3918\n",
      "using time 0.7059s, training loss at epoch 267: 0.4001\n",
      "using time 0.7130s, training loss at epoch 268: 0.3939\n",
      "using time 0.6726s, training loss at epoch 269: 0.3951\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  270  | 0.7052836418151855 | 2.9991419315338135 | 0.3852875530719757 | [0.03746183] | [0.02013276] | [0.00525542] | [0.07879636] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7220s, training loss at epoch 271: 0.3915\n",
      "using time 0.7048s, training loss at epoch 272: 0.3974\n",
      "using time 0.7323s, training loss at epoch 273: 0.3752\n",
      "using time 0.7154s, training loss at epoch 274: 0.3789\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|  275  | 0.7014434337615967 | 3.0388433933258057 | 0.3886082172393799 | [0.03805704] | [0.02029644] | [0.00530441] | [0.07949615] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7637s, training loss at epoch 276: 0.3917\n",
      "using time 0.7327s, training loss at epoch 277: 0.4002\n",
      "using time 0.6973s, training loss at epoch 278: 0.3825\n",
      "using time 0.7080s, training loss at epoch 279: 0.3749\n",
      "+-------+--------------------+-------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |    ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "|  280  | 0.7104349136352539 | 3.087730646133423 | 0.3759279251098633 | [0.03746404] | [0.020057] | [0.00522743] | [0.07809657] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+------------+--------------+--------------+\n",
      "using time 0.7635s, training loss at epoch 281: 0.3757\n",
      "using time 0.6916s, training loss at epoch 282: 0.3796\n",
      "using time 0.7236s, training loss at epoch 283: 0.3744\n",
      "using time 0.7066s, training loss at epoch 284: 0.3590\n",
      "+-------+-------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)  |         Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "|  285  | 0.724461555480957 | 3.019900321960449 | 0.36114442348480225 | [0.03742756] | [0.02013158] | [0.00524843] | [0.07837649] |\n",
      "+-------+-------------------+-------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7000s, training loss at epoch 286: 0.3593\n",
      "using time 0.7083s, training loss at epoch 287: 0.3669\n",
      "using time 0.7048s, training loss at epoch 288: 0.3651\n",
      "using time 0.7132s, training loss at epoch 289: 0.3672\n",
      "+-------+--------------------+------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |  tesing time(s)  |         Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "|  290  | 0.6881494522094727 | 2.99018931388855 | 0.37537291646003723 | [0.03748058] | [0.02013267] | [0.00524843] | [0.07809657] |\n",
      "+-------+--------------------+------------------+---------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7901s, training loss at epoch 291: 0.3614\n",
      "using time 0.7009s, training loss at epoch 292: 0.3641\n",
      "using time 0.6854s, training loss at epoch 293: 0.3518\n",
      "using time 0.7054s, training loss at epoch 294: 0.3583\n",
      "+-------+--------------------+-------------------+---------------------+--------------+--------------+--------------+-------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |         Loss        |    recall    |     ndcg     |  precision   |  hit_ratio  |\n",
      "+-------+--------------------+-------------------+---------------------+--------------+--------------+--------------+-------------+\n",
      "|  295  | 0.7206518650054932 | 3.138683319091797 | 0.36209625005722046 | [0.03746871] | [0.02009125] | [0.00521344] | [0.0776767] |\n",
      "+-------+--------------------+-------------------+---------------------+--------------+--------------+--------------+-------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time 0.6761s, training loss at epoch 296: 0.3316\n",
      "using time 0.6753s, training loss at epoch 297: 0.3687\n",
      "using time 0.7153s, training loss at epoch 298: 0.3650\n",
      "using time 0.6874s, training loss at epoch 299: 0.3397\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+-------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio  |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+-------------+\n",
      "|  300  | 0.7071404457092285 | 3.1493520736694336 | 0.3558851480484009 | [0.03765801] | [0.02021433] | [0.00526242] | [0.0786564] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+-------------+\n",
      "using time 0.7028s, training loss at epoch 301: 0.3524\n",
      "using time 0.6797s, training loss at epoch 302: 0.3462\n",
      "using time 0.6870s, training loss at epoch 303: 0.3564\n",
      "using time 0.6783s, training loss at epoch 304: 0.3441\n",
      "+-------+--------------------+--------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg    |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "|  305  | 0.7052140235900879 | 3.0065042972564697 | 0.3527832627296448 | [0.03759018] | [0.0200752] | [0.00523443] | [0.07809657] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+-------------+--------------+--------------+\n",
      "Early stopping is trigger at step: 10 log:0.03759018161412604\n",
      "early stopping at 305, recall@20:0.0385\n"
     ]
    }
   ],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from utils.parser import parse_args\n",
    "from utils.data_loader import load_data\n",
    "from utils.evaluate import test\n",
    "from utils.helper import early_stopping\n",
    "\n",
    "n_users = 0\n",
    "n_items = 0\n",
    "\n",
    "\n",
    "def get_feed_dict(train_entity_pairs, train_pos_set, start, end, n_negs=1):\n",
    "\n",
    "    def sampling(user_item, train_set, n):\n",
    "        neg_items = []\n",
    "        for user, _ in user_item.cpu().numpy():\n",
    "            user = int(user)\n",
    "            negitems = []\n",
    "            for i in range(n):  # sample n times\n",
    "                while True:\n",
    "                    negitem = random.choice(range(n_items))\n",
    "                    if negitem not in train_set[user]:\n",
    "                        break\n",
    "                negitems.append(negitem)\n",
    "            neg_items.append(negitems)\n",
    "        return neg_items\n",
    "\n",
    "    feed_dict = {}\n",
    "    entity_pairs = train_entity_pairs[start:end]\n",
    "    feed_dict['users'] = entity_pairs[:, 0]\n",
    "    feed_dict['pos_items'] = entity_pairs[:, 1]\n",
    "    feed_dict['neg_items'] = torch.LongTensor(sampling(entity_pairs,\n",
    "                                                       train_pos_set,\n",
    "                                                       n_negs*K)).to(device)\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"fix the random seed\"\"\"\n",
    "    seed = 2020\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    \"\"\"read args\"\"\"\n",
    "    global args, device\n",
    "    args = parse_args()\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "    device = torch.device(\"cuda:0\") if args.cuda else torch.device(\"cpu\")\n",
    "\n",
    "    \"\"\"build dataset\"\"\"\n",
    "    train_cf, user_dict, n_params, norm_mat = load_data(args)\n",
    "    train_cf_size = len(train_cf)\n",
    "    train_cf = torch.LongTensor(np.array([[cf[0], cf[1]] for cf in train_cf], np.int32))\n",
    "\n",
    "    n_users = n_params['n_users']\n",
    "    n_items = n_params['n_items']\n",
    "    n_negs = args.n_negs\n",
    "    K = args.K\n",
    "\n",
    "    \"\"\"define model\"\"\"\n",
    "    from modules.LightGCN import LightGCN\n",
    "    if args.gnn == 'lightgcn':\n",
    "        model = LightGCN(n_params, args, norm_mat).to(device)\n",
    "\n",
    "    \"\"\"define optimizer\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    cur_best_pre_0 = 0\n",
    "    stopping_step = 0\n",
    "    should_stop = False\n",
    "\n",
    "    print(\"start training ...\")\n",
    "    for epoch in range(args.epoch):\n",
    "        # shuffle training data\n",
    "        train_cf_ = train_cf\n",
    "        index = np.arange(len(train_cf_))\n",
    "        np.random.shuffle(index)\n",
    "        train_cf_ = train_cf_[index].to(device)\n",
    "\n",
    "        \"\"\"training\"\"\"\n",
    "        model.train()\n",
    "        loss, s = 0, 0\n",
    "        hits = 0\n",
    "        train_s_t = time()\n",
    "        while s + args.batch_size <= len(train_cf):\n",
    "            batch = get_feed_dict(train_cf_,\n",
    "                                  user_dict['train_user_set'],\n",
    "                                  s, s + args.batch_size,\n",
    "                                  n_negs)\n",
    "\n",
    "            batch_loss, _, _ = model(epoch, batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += batch_loss\n",
    "            s += args.batch_size\n",
    "\n",
    "        train_e_t = time()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            \"\"\"testing\"\"\"\n",
    "\n",
    "            train_res = PrettyTable()\n",
    "            train_res.field_names = [\"Epoch\", \"training time(s)\", \"tesing time(s)\", \"Loss\", \"recall\", \"ndcg\", \"precision\", \"hit_ratio\"]\n",
    "\n",
    "            model.eval()\n",
    "            test_s_t = time()\n",
    "            test_ret = test(model, user_dict, n_params, mode='test')\n",
    "            test_e_t = time()\n",
    "            train_res.add_row(\n",
    "                [epoch, train_e_t - train_s_t, test_e_t - test_s_t, loss.item(), test_ret['recall'], test_ret['ndcg'],\n",
    "                 test_ret['precision'], test_ret['hit_ratio']])\n",
    "\n",
    "            if user_dict['valid_user_set'] is None:\n",
    "                valid_ret = test_ret\n",
    "            else:\n",
    "                test_s_t = time()\n",
    "                valid_ret = test(model, user_dict, n_params, mode='valid')\n",
    "                test_e_t = time()\n",
    "                train_res.add_row(\n",
    "                    [epoch, train_e_t - train_s_t, test_e_t - test_s_t, loss.item(), valid_ret['recall'], valid_ret['ndcg'],\n",
    "                     valid_ret['precision'], valid_ret['hit_ratio']])\n",
    "            print(train_res)\n",
    "\n",
    "            # *********************************************************\n",
    "            # early stopping when cur_best_pre_0 is decreasing for 10 successive steps.\n",
    "            cur_best_pre_0, stopping_step, should_stop = early_stopping(valid_ret['recall'][0], cur_best_pre_0,\n",
    "                                                                        stopping_step, expected_order='acc',\n",
    "                                                                        flag_step=10)\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            \"\"\"save weight\"\"\"\n",
    "            if valid_ret['recall'][0] == cur_best_pre_0 and args.save:\n",
    "                torch.save(model.state_dict(), args.out_dir + 'model_' + '.ckpt')\n",
    "        else:\n",
    "            # logging.info('training loss at epoch %d: %f' % (epoch, loss.item()))\n",
    "            print('using time %.4fs, training loss at epoch %d: %.4f' % (train_e_t - train_s_t, epoch, loss.item()))\n",
    "\n",
    "    print('early stopping at %d, recall@20:%.4f' % (epoch, cur_best_pre_0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
