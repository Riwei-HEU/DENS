{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading train and test user-item set ...\n",
      "building the adj mat ...\n",
      "{'n_users': 4626, 'n_items': 10083}\n",
      "loading over ...\n",
      "start training ...\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   0   | 0.9738812446594238 | 1.9395687580108643 | 14.549932479858398 | [0.02030559] | [0.01137201] | [0.00206456] | [0.03303303] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.9740s, training loss at epoch 1: 14.5248\n",
      "using time 0.8868s, training loss at epoch 2: 14.4557\n",
      "using time 0.9135s, training loss at epoch 3: 14.3129\n",
      "using time 0.8728s, training loss at epoch 4: 14.0848\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision  |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "|   5   | 0.8399801254272461 | 1.8873283863067627 | 13.794087409973145 | [0.03944345] | [0.02151171] | [0.0043043] | [0.07132132] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+-------------+--------------+\n",
      "using time 0.8830s, training loss at epoch 6: 13.4519\n",
      "using time 0.8863s, training loss at epoch 7: 13.1059\n",
      "using time 0.8485s, training loss at epoch 8: 12.7808\n",
      "using time 0.8545s, training loss at epoch 9: 12.4314\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   10  | 0.8523316383361816 | 1.8654100894927979 | 12.141810417175293 | [0.04200354] | [0.02287749] | [0.00467968] | [0.07757758] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8621s, training loss at epoch 11: 11.8074\n",
      "using time 0.8538s, training loss at epoch 12: 11.4877\n",
      "using time 0.8592s, training loss at epoch 13: 11.2038\n",
      "using time 0.8395s, training loss at epoch 14: 10.9314\n",
      "+-------+-------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   15  | 0.850135326385498 | 1.8705780506134033 | 10.614380836486816 | [0.04435759] | [0.02436686] | [0.00486737] | [0.08108108] |\n",
      "+-------+-------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8690s, training loss at epoch 16: 10.2782\n",
      "using time 0.8514s, training loss at epoch 17: 10.0306\n",
      "using time 0.8429s, training loss at epoch 18: 9.7647\n",
      "using time 0.8456s, training loss at epoch 19: 9.4524\n",
      "+-------+-------------------+-------------------+------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)  |       Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+-------------------+------------------+--------------+--------------+--------------+--------------+\n",
      "|   20  | 0.849034309387207 | 1.814573049545288 | 9.19758129119873 | [0.04573962] | [0.02444316] | [0.00499249] | [0.08358358] |\n",
      "+-------+-------------------+-------------------+------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8609s, training loss at epoch 21: 8.9561\n",
      "using time 0.8518s, training loss at epoch 22: 8.6509\n",
      "using time 0.8408s, training loss at epoch 23: 8.3958\n",
      "using time 0.8446s, training loss at epoch 24: 8.1507\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   25  | 0.8439421653747559 | 1.8227722644805908 | 7.8999857902526855 | [0.04765543] | [0.02520716] | [0.00513013] | [0.08608609] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8740s, training loss at epoch 26: 7.7265\n",
      "using time 0.8422s, training loss at epoch 27: 7.4603\n",
      "using time 0.8495s, training loss at epoch 28: 7.2558\n",
      "using time 0.9011s, training loss at epoch 29: 7.0820\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   30  | 0.8509080410003662 | 1.8734712600708008 | 6.869433879852295 | [0.04705047] | [0.02476141] | [0.00503003] | [0.08458458] |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8732s, training loss at epoch 31: 6.6743\n",
      "using time 0.8532s, training loss at epoch 32: 6.5222\n",
      "using time 0.8463s, training loss at epoch 33: 6.3214\n",
      "using time 0.8610s, training loss at epoch 34: 6.1606\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   35  | 0.8510630130767822 | 1.8816702365875244 | 5.954773426055908 | [0.04577419] | [0.02447033] | [0.00492993] | [0.08308308] |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.7788s, training loss at epoch 36: 5.8432\n",
      "using time 0.7438s, training loss at epoch 37: 5.7330\n",
      "using time 0.7362s, training loss at epoch 38: 5.5646\n",
      "using time 0.9073s, training loss at epoch 39: 5.3714\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   40  | 0.9435074329376221 | 2.006286859512329 | 5.301419258117676 | [0.04665354] | [0.02467651] | [0.00500501] | [0.08358358] |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.9509s, training loss at epoch 41: 5.1799\n",
      "using time 0.9170s, training loss at epoch 42: 5.0565\n",
      "using time 0.8957s, training loss at epoch 43: 4.9468\n",
      "using time 0.9303s, training loss at epoch 44: 4.8415\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   45  | 0.9403297901153564 | 2.356436252593994 | 4.708518981933594 | [0.04540636] | [0.02409781] | [0.00486737] | [0.08083083] |\n",
      "+-------+--------------------+-------------------+-------------------+--------------+--------------+--------------+--------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time 0.9604s, training loss at epoch 46: 4.5931\n",
      "using time 0.9154s, training loss at epoch 47: 4.4813\n",
      "using time 0.9811s, training loss at epoch 48: 4.4277\n",
      "using time 0.9427s, training loss at epoch 49: 4.3010\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   50  | 0.8690590858459473 | 2.1675214767456055 | 4.2130231857299805 | [0.04554936] | [0.02391321] | [0.00489239] | [0.08108108] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 1.1030s, training loss at epoch 51: 4.1439\n",
      "using time 0.9793s, training loss at epoch 52: 4.0785\n",
      "using time 0.9520s, training loss at epoch 53: 3.9565\n",
      "using time 1.0305s, training loss at epoch 54: 3.8932\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   55  | 0.9248206615447998 | 2.029360055923462 | 3.8278660774230957 | [0.04585559] | [0.02398388] | [0.00485485] | [0.08083083] |\n",
      "+-------+--------------------+-------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.9401s, training loss at epoch 56: 3.7472\n",
      "using time 0.9344s, training loss at epoch 57: 3.6776\n",
      "using time 0.9023s, training loss at epoch 58: 3.5918\n",
      "using time 0.9268s, training loss at epoch 59: 3.5869\n",
      "+-------+-------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s) |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+-------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   60  | 0.933680534362793 | 1.9998040199279785 | 3.4807653427124023 | [0.04531159] | [0.02400164] | [0.00481732] | [0.08008008] |\n",
      "+-------+-------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "using time 1.0507s, training loss at epoch 61: 3.4099\n",
      "using time 0.9536s, training loss at epoch 62: 3.3572\n",
      "using time 0.9120s, training loss at epoch 63: 3.2958\n",
      "using time 0.9272s, training loss at epoch 64: 3.2789\n",
      "+-------+--------------------+-------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |    recall   |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+-------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "|   65  | 0.9653208255767822 | 1.915022373199463 | 3.1510870456695557 | [0.0449201] | [0.02396803] | [0.00481732] | [0.07957958] |\n",
      "+-------+--------------------+-------------------+--------------------+-------------+--------------+--------------+--------------+\n",
      "using time 0.9306s, training loss at epoch 66: 3.1056\n",
      "using time 0.9939s, training loss at epoch 67: 3.0744\n",
      "using time 0.9175s, training loss at epoch 68: 3.0051\n",
      "using time 0.9510s, training loss at epoch 69: 2.9781\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|   70  | 0.9620354175567627 | 1.8919405937194824 | 2.885094404220581 | [0.04470792] | [0.02387577] | [0.00475475] | [0.07882883] |\n",
      "+-------+--------------------+--------------------+-------------------+--------------+--------------+--------------+--------------+\n",
      "using time 0.8992s, training loss at epoch 71: 2.8700\n",
      "using time 0.9111s, training loss at epoch 72: 2.8027\n",
      "using time 0.9330s, training loss at epoch 73: 2.7764\n",
      "using time 0.9285s, training loss at epoch 74: 2.7033\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |    recall    |     ndcg     |  precision   |  hit_ratio   |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "|   75  | 0.9489326477050781 | 1.9774985313415527 | 2.6980128288269043 | [0.04523613] | [0.02411609] | [0.00476727] | [0.07932933] |\n",
      "+-------+--------------------+--------------------+--------------------+--------------+--------------+--------------+--------------+\n",
      "Early stopping is trigger at step: 10 log:0.04523612659577579\n",
      "early stopping at 75, recall@20:0.0477\n"
     ]
    }
   ],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from utils.parser import parse_args\n",
    "from utils.data_loader import load_data\n",
    "from utils.evaluate import test\n",
    "from utils.helper import early_stopping\n",
    "\n",
    "n_users = 0\n",
    "n_items = 0\n",
    "\n",
    "\n",
    "def get_feed_dict(train_entity_pairs, train_pos_set, start, end, n_negs=1):\n",
    "\n",
    "    def sampling(user_item, train_set, n):\n",
    "        neg_items = []\n",
    "        for user, _ in user_item.cpu().numpy():\n",
    "            user = int(user)\n",
    "            negitems = []\n",
    "            for i in range(n):  # sample n times\n",
    "                while True:\n",
    "                    negitem = random.choice(range(n_items))\n",
    "                    if negitem not in train_set[user]:\n",
    "                        break\n",
    "                negitems.append(negitem)\n",
    "            neg_items.append(negitems)\n",
    "        return neg_items\n",
    "\n",
    "    feed_dict = {}\n",
    "    entity_pairs = train_entity_pairs[start:end]\n",
    "    feed_dict['users'] = entity_pairs[:, 0]\n",
    "    feed_dict['pos_items'] = entity_pairs[:, 1]\n",
    "    feed_dict['neg_items'] = torch.LongTensor(sampling(entity_pairs,\n",
    "                                                       train_pos_set,\n",
    "                                                       n_negs*K)).to(device)\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"fix the random seed\"\"\"\n",
    "    seed = 2020\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    \"\"\"read args\"\"\"\n",
    "    global args, device\n",
    "    args = parse_args()\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)\n",
    "    device = torch.device(\"cuda:0\") if args.cuda else torch.device(\"cpu\")\n",
    "\n",
    "    \"\"\"build dataset\"\"\"\n",
    "    train_cf, user_dict, n_params, norm_mat = load_data(args)\n",
    "    train_cf_size = len(train_cf)\n",
    "    train_cf = torch.LongTensor(np.array([[cf[0], cf[1]] for cf in train_cf], np.int32))\n",
    "\n",
    "    n_users = n_params['n_users']\n",
    "    n_items = n_params['n_items']\n",
    "    n_negs = args.n_negs\n",
    "    K = args.K\n",
    "\n",
    "    \"\"\"define model\"\"\"\n",
    "    from modules.LightGCN import LightGCN\n",
    "    if args.gnn == 'lightgcn':\n",
    "        model = LightGCN(n_params, args, norm_mat).to(device)\n",
    "\n",
    "    \"\"\"define optimizer\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    cur_best_pre_0 = 0\n",
    "    stopping_step = 0\n",
    "    should_stop = False\n",
    "\n",
    "    print(\"start training ...\")\n",
    "    for epoch in range(args.epoch):\n",
    "        # shuffle training data\n",
    "        train_cf_ = train_cf\n",
    "        index = np.arange(len(train_cf_))\n",
    "        np.random.shuffle(index)\n",
    "        train_cf_ = train_cf_[index].to(device)\n",
    "\n",
    "        \"\"\"training\"\"\"\n",
    "        model.train()\n",
    "        loss, s = 0, 0\n",
    "        hits = 0\n",
    "        train_s_t = time()\n",
    "        while s + args.batch_size <= len(train_cf):\n",
    "            batch = get_feed_dict(train_cf_,\n",
    "                                  user_dict['train_user_set'],\n",
    "                                  s, s + args.batch_size,\n",
    "                                  n_negs)\n",
    "\n",
    "            batch_loss, _, _ = model(epoch, batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += batch_loss\n",
    "            s += args.batch_size\n",
    "\n",
    "        train_e_t = time()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            \"\"\"testing\"\"\"\n",
    "\n",
    "            train_res = PrettyTable()\n",
    "            train_res.field_names = [\"Epoch\", \"training time(s)\", \"tesing time(s)\", \"Loss\", \"recall\", \"ndcg\", \"precision\", \"hit_ratio\"]\n",
    "\n",
    "            model.eval()\n",
    "            test_s_t = time()\n",
    "            test_ret = test(model, user_dict, n_params, mode='test')\n",
    "            test_e_t = time()\n",
    "            train_res.add_row(\n",
    "                [epoch, train_e_t - train_s_t, test_e_t - test_s_t, loss.item(), test_ret['recall'], test_ret['ndcg'],\n",
    "                 test_ret['precision'], test_ret['hit_ratio']])\n",
    "\n",
    "            if user_dict['valid_user_set'] is None:\n",
    "                valid_ret = test_ret\n",
    "            else:\n",
    "                test_s_t = time()\n",
    "                valid_ret = test(model, user_dict, n_params, mode='valid')\n",
    "                test_e_t = time()\n",
    "                train_res.add_row(\n",
    "                    [epoch, train_e_t - train_s_t, test_e_t - test_s_t, loss.item(), valid_ret['recall'], valid_ret['ndcg'],\n",
    "                     valid_ret['precision'], valid_ret['hit_ratio']])\n",
    "            print(train_res)\n",
    "\n",
    "            # *********************************************************\n",
    "            # early stopping when cur_best_pre_0 is decreasing for 10 successive steps.\n",
    "            cur_best_pre_0, stopping_step, should_stop = early_stopping(valid_ret['recall'][0], cur_best_pre_0,\n",
    "                                                                        stopping_step, expected_order='acc',\n",
    "                                                                        flag_step=10)\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            \"\"\"save weight\"\"\"\n",
    "            if valid_ret['recall'][0] == cur_best_pre_0 and args.save:\n",
    "                torch.save(model.state_dict(), args.out_dir + 'model_' + '.ckpt')\n",
    "        else:\n",
    "            # logging.info('training loss at epoch %d: %f' % (epoch, loss.item()))\n",
    "            print('using time %.4fs, training loss at epoch %d: %.4f' % (train_e_t - train_s_t, epoch, loss.item()))\n",
    "\n",
    "    print('early stopping at %d, recall@20:%.4f' % (epoch, cur_best_pre_0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
